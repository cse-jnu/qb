
### **1. a)** For each of the questions below, circle either **T** (True) or **F** (False):

i. **Kruskal’s and Prim’s algorithms always generate the same minimum spanning tree.**
**Answer:** **F**
*Explanation:* They generate a minimum spanning tree (MST), but if the weights are not unique, the MST may not be the same.

ii. **It is possible to sort any set of numbers in linear order of time, i.e. O(n).**
**Answer:** **F**
*Explanation:* Comparison-based sorting cannot be faster than O(n log n). Linear sorting (like counting sort) works only for specific cases like small integer ranges.

iii. **We typically apply dynamic programming to optimization problems.**
**Answer:** **T**
*Explanation:* Dynamic programming is usually applied to optimization problems where the solution depends on solutions to subproblems.

iv. **The height of a complete binary tree with N nodes is at most O(log N).**
**Answer:** **T**
*Explanation:* In a complete binary tree, the height is log₂(N), so O(log N) is correct.

 

### **1. b)** Write a procedure to solve the coin changing problem.

**Answer:**

**Problem:** Given coins of certain denominations and a total amount, find the minimum number of coins that make up the amount.

**Dynamic Programming Approach:**

```python
def coinChange(coins, amount):
    dp = [float('inf')] * (amount + 1)
    dp[0] = 0

    for i in range(1, amount + 1):
        for coin in coins:
            if coin <= i:
                dp[i] = min(dp[i], dp[i - coin] + 1)

    return dp[amount] if dp[amount] != float('inf') else -1
```

Example:
`coins = [1, 2, 5], amount = 11 → Output = 3 (5+5+1)`

 

### **1. c)** What is a greedy algorithm? When can we choose a greedy algorithm?

**Answer:**

A greedy algorithm builds up a solution piece by piece, always choosing the next piece that offers the most immediate benefit (locally optimal choice), hoping this leads to a globally optimal solution.

**When to choose:**

* When the problem has the **greedy-choice property** (local choice leads to global optimum).
* When the problem exhibits **optimal substructure** (optimal solution contains optimal solutions to subproblems).

**Example:** Activity selection, Huffman coding, Kruskal’s algorithm.

 

### **1. d)** What is amortized analysis? Explain accounting method with proper example.

**Answer:**

**Amortized analysis** gives the average time per operation over a sequence of operations, even if some operations are expensive.

**Accounting method:** Assign an artificial “charge” (credit) to operations.

**Example:** Dynamic array resizing

* Suppose we double the array size when full.
* Most inserts are O(1), but resizing is O(n).
* By charging each insert 3 units, we can "save up" credits to pay for resizing.
* This shows average cost per insert is still O(1).

 

### **2. a)** DNA sequence problem (Longest Common Subsequence)

Mother: `AACCGGCCCCCTT`
Child:  `ATCGATCGCGCGAT`

We solve it using Dynamic Programming (DP).

**Table (dp\[i]\[j]):** Each cell represents the length of LCS up to position i (mother) and j (child).
If characters match:
`dp[i][j] = dp[i-1][j-1] + 1`
Else:
`dp[i][j] = max(dp[i-1][j], dp[i][j-1])`

Below is the full DP table for

$$
X = \text{A A C C G G C C C C C C T T},\quad m=14
$$

$$
Y = \text{A T C G A T C G C G C G A T},\quad n=14
$$

| X\Y   |  0  |  A  |  T  |  C  |  G  |  A  |  T  |  C  |  G  |  C  |  G  |  C  |  G  |  A  |  T  |
|:-----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| **0** |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |
| **A** |  0  |  1  |  1  |  1  |  1  |  1  |  1  |  1  |  1  |  1  |  1  |  1  |  1  |  1  |  1  |
| **A** |  0  |  1  |  1  |  1  |  1  |  2  |  2  |  2  |  2  |  2  |  2  |  2  |  2  |  2  |  2  |
| **C** |  0  |  1  |  1  |  2  |  2  |  2  |  2  |  3  |  3  |  3  |  3  |  3  |  3  |  3  |  3  |
| **C** |  0  |  1  |  1  |  2  |  2  |  2  |  2  |  3  |  3  |  4  |  4  |  4  |  4  |  4  |  4  |
| **G** |  0  |  1  |  1  |  2  |  3  |  3  |  3  |  3  |  4  |  4  |  5  |  5  |  5  |  5  |  5  |
| **G** |  0  |  1  |  1  |  2  |  3  |  3  |  3  |  3  |  4  |  4  |  5  |  5  |  6  |  6  |  6  |
| **C** |  0  |  1  |  1  |  2  |  3  |  3  |  3  |  4  |  4  |  5  |  5  |  6  |  6  |  6  |  6  |
| **C** |  0  |  1  |  1  |  2  |  3  |  3  |  3  |  4  |  4  |  5  |  5  |  6  |  6  |  6  |  6  |
| **C** |  0  |  1  |  1  |  2  |  3  |  3  |  3  |  4  |  4  |  5  |  5  |  6  |  6  |  6  |  6  |
| **C** |  0  |  1  |  1  |  2  |  3  |  3  |  3  |  4  |  4  |  5  |  5  |  6  |  6  |  6  |  6  |
| **C** |  0  |  1  |  1  |  2  |  3  |  3  |  3  |  4  |  4  |  5  |  5  |  6  |  6  |  6  |  6  |
| **C** |  0  |  1  |  1  |  2  |  3  |  3  |  3  |  4  |  4  |  5  |  5  |  6  |  6  |  6  |  6  |
| **T** |  0  |  1  |  2  |  2  |  3  |  3  |  4  |  4  |  4  |  5  |  5  |  6  |  6  |  6  |  7  |
| **T** |  0  |  1  |  2  |  2  |  3  |  3  |  4  |  4  |  4  |  5  |  5  |  6  |  6  |  6  |  7  |


- The bottom-right cell **c\[14]\[14] = 7**, so the **LCS length is 7**.
- One possible LCS (back-tracking) is

```
A A C C G G T
```

 
 

### **2. b)** Matrix Chain Multiplication


### Matrix Dimensions:

* A = 3×2
* B = 2×4
* C = 4×2
* D = 2×5

Dimension array → `p = [3, 2, 4, 2, 5]`

 

### DP Table **m\[i]\[j]**

`m[i][j]` stores minimum cost (scalar multiplications) to multiply matrices from **Ai to Aj**.

| i\j | 1 | 2  | 3  | 4  |
| ---  | - | -- | -- | -- |
| 1   | 0 | 24 | 28 | 58 |
| 2   |   | 0  | 16 | 36 |
| 3   |   |    | 0  | 40 |
| 4   |   |    |    | 0  |

 

### **Computation Details**

**m\[1]\[2]:**
Multiply A(3x2) and B(2x4):
`3 * 2 * 4 = 24`

**m\[2]\[3]:**
Multiply B(2x4) and C(4x2):
`2 * 4 * 2 = 16`

**m\[3]\[4]:**
Multiply C(4x2) and D(2x5):
`4 * 2 * 5 = 40`

 

**m\[1]\[3]:** (A(BC)) vs ((AB)C)

* (A)(BC): `0 + 16 + 3*2*2 = 28`
* (AB)(C): `24 + 0 + 3*4*2 = 48`

→ min = **28**

 

**m\[2]\[4]:** (B(CD)) vs ((BC)D)

* (B)(CD): `0 + 40 + 2*4*5 = 80`
* (BC)(D): `16 + 0 + 2*2*5 = 36`

→ min = **36**

 

**m\[1]\[4]:** ((A(BC))D), (A)((BCD)), ((AB)(CD))

* k=1: `(A)(BCD)` → `0 + 36 + 3*2*5 = 66`
* k=2: `(AB)(CD)` → `24 + 40 + 3*4*5 = 124`
* k=3: `(ABC)(D)` → `28 + 0 + 3*2*5 = 58`

→ min = **58**

 

### **Optimal Parenthesization**

From the DP table:

* **m\[1]\[4]** → split at **k=3** → `((A(BC))D)`

 

### Final Answer:

✅ Minimum number of scalar multiplications = **58**
✅ Optimal parenthesization = `((A(BC))D)`

 


### **2. c)** Briefly explain O-notation and Θ-notation with appropriate figure.

**O-notation (Big-O):**

* Upper bound of algorithm runtime.
* T(n) = O(f(n)) → For large n, runtime does not exceed c\*f(n).

**Θ-notation (Theta):**

* Tight bound (both upper and lower).
* T(n) = Θ(f(n)) → For large n, runtime grows exactly like f(n).

**Figure (conceptual):**

```
T(n)
|
|        Θ(n log n)
|        --
|     /        \
| -/          \ -  O(n^2)
|
|_________________________ n →
```


### **5. a) Comparison between BFS and DFS**

| Criteria                  | **BFS (Breadth-First Search)**                                                                                  | **DFS (Depth-First Search)**                                                                                   |
| ------------------------- | --------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------- |
| **Fundamental Principle** | Explores neighbors level by level (layer-wise)                                                                  | Explores as far as possible along each branch before backtracking                                              |
| **Traversal Order**       | Level-order                                                                                                     | Depth-order                                                                                                    |
| **Data Structure Used**   | Queue (FIFO)                                                                                                    | Stack (can be implemented with recursion or explicit stack)                                                    |
| **Visited Marking**       | Mark nodes as visited when enqueued                                                                             | Mark nodes as visited when first visited                                                                       |
| **Time Complexity**       | O(V + E)                                                                                                        | O(V + E)                                                                                                       |
| **Space Complexity**      | O(V) for queue and visited array                                                                                | O(V) for recursion stack and visited array                                                                     |
| **Applications**          | - Shortest path in unweighted graphs  <br> - Finding connected components <br> - Level-order traversal in trees | - Topological sorting <br> - Detecting cycles in graphs <br> - Solving puzzles like maze/backtracking problems |
| **Example**               | Shortest path in social networks                                                                                | Solving a maze, scheduling problems                                                                            |

---

### **5. b) Strongly Connected Components (SCC) & Acyclic Component Graph**


### **Kosaraju’s Algorithm: Steps**

1️⃣ **Do a DFS on original graph G**
→ Store nodes in order of **finishing times** (stack).

2️⃣ **Reverse the graph** (transpose of G)

3️⃣ **Do DFS on the transposed graph**, using nodes in order of the stack.
→ Each DFS traversal gives one **SCC**.

---

### **Step 1: DFS on original graph (G)**

Graph nodes:
{ A, B, C, D, E, F, G, H, I, J, K }

DFS traversal (one possible order):
→ Let’s say we explore from node A:

```
A → C → F → D → I → H → K → J → B → E → G
```

When node finishes, push it onto stack:

Example stack after DFS:

| Stack (Top to Bottom) |
| --------------------- |
| G                     |
| B                     |
| E                     |
| J                     |
| K                     |
| H                     |
| I                     |
| D                     |
| F                     |
| C                     |
| A                     |

---

### **Step 2: Reverse the graph**

Reverse all edges:

* A ← C
* F ← C
* C ← F
* D ← C
* I ← D
* H ← D
* K ← E
* J ← E
* G ← A
* E ← B
* B ← A
* D ← C
* B ← C
* I ← H
* H ← I

---

### **Step 3: DFS on transposed graph (in stack order)**

Now process nodes in stack order:

1️⃣ Start with G → no outgoing edge → SCC = {G}

2️⃣ B → SCC = {B}

3️⃣ E → SCC = {E}

4️⃣ J → SCC = {J}

5️⃣ K → SCC = {K}

6️⃣ H → DFS finds I → H ↔ I → SCC = {H, I}

7️⃣ F → DFS finds C → F ↔ C → SCC = {F, C}

8️⃣ D → SCC = {D}

9️⃣ A → SCC = {A}

---

### **Final SCCs**

* {F, C}
* {H, I}
* {A}
* {B}
* {D}
* {E}
* {G}
* {J}
* {K}

---

✅ This matches what we used earlier for drawing the **contracted acyclic graph**.

---

### **Summary of Kosaraju’s Algorithm**

| Step                | Action                   |
| ------------------- | ------------------------ |
| Step 1 (DFS)        | Get finish order (stack) |
| Step 2 (Reverse)    | Reverse edges            |
| Step 3 (DFS on G^T) | Find SCCs                |
| Time Complexity     | **O(V + E)**             |



![image](https://github.com/user-attachments/assets/1b091a92-2833-4e2e-acbb-817509f4c839)


