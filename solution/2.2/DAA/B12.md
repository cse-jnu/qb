
### **1. a)** For each of the questions below, circle either **T** (True) or **F** (False):

i. **Kruskal’s and Prim’s algorithms always generate the same minimum spanning tree.**
**Answer:** **F**
*Explanation:* They generate a minimum spanning tree (MST), but if the weights are not unique, the MST may not be the same.

ii. **It is possible to sort any set of numbers in linear order of time, i.e. O(n).**
**Answer:** **F**
*Explanation:* Comparison-based sorting cannot be faster than O(n log n). Linear sorting (like counting sort) works only for specific cases like small integer ranges.

iii. **We typically apply dynamic programming to optimization problems.**
**Answer:** **T**
*Explanation:* Dynamic programming is usually applied to optimization problems where the solution depends on solutions to subproblems.

iv. **The height of a complete binary tree with N nodes is at most O(log N).**
**Answer:** **T**
*Explanation:* In a complete binary tree, the height is log₂(N), so O(log N) is correct.

 

### **1. b)** Write a procedure to solve the coin changing problem.

**Answer:**

**Problem:** Given coins of certain denominations and a total amount, find the minimum number of coins that make up the amount.

**Dynamic Programming Approach:**

```python
def coinChange(coins, amount):
    dp = [float('inf')] * (amount + 1)
    dp[0] = 0

    for i in range(1, amount + 1):
        for coin in coins:
            if coin <= i:
                dp[i] = min(dp[i], dp[i - coin] + 1)

    return dp[amount] if dp[amount] != float('inf') else -1
```

Example:
`coins = [1, 2, 5], amount = 11 → Output = 3 (5+5+1)`

 

### **1. c)** What is a greedy algorithm? When can we choose a greedy algorithm?

**Answer:**

A greedy algorithm builds up a solution piece by piece, always choosing the next piece that offers the most immediate benefit (locally optimal choice), hoping this leads to a globally optimal solution.

**When to choose:**

* When the problem has the **greedy-choice property** (local choice leads to global optimum).
* When the problem exhibits **optimal substructure** (optimal solution contains optimal solutions to subproblems).

**Example:** Activity selection, Huffman coding, Kruskal’s algorithm.

 

### **1. d)** What is amortized analysis? Explain accounting method with proper example.

**Answer:**

**Amortized analysis** gives the average time per operation over a sequence of operations, even if some operations are expensive.

**Accounting method:** Assign an artificial “charge” (credit) to operations.

**Example:** Dynamic array resizing

* Suppose we double the array size when full.
* Most inserts are O(1), but resizing is O(n).
* By charging each insert 3 units, we can "save up" credits to pay for resizing.
* This shows average cost per insert is still O(1).

 

### **2. a)** DNA sequence problem (Longest Common Subsequence)

Mother: `AACCGGCCCCCTT`
Child:  `ATCGATCGCGCGAT`

We solve it using Dynamic Programming (DP).

**Table (dp\[i]\[j]):** Each cell represents the length of LCS up to position i (mother) and j (child).
If characters match:
`dp[i][j] = dp[i-1][j-1] + 1`
Else:
`dp[i][j] = max(dp[i-1][j], dp[i][j-1])`

Below is the full DP table for

$$
X = \text{A A C C G G C C C C C C T T},\quad m=14
$$

$$
Y = \text{A T C G A T C G C G C G A T},\quad n=14
$$

| X\Y   |  0  |  A  |  T  |  C  |  G  |  A  |  T  |  C  |  G  |  C  |  G  |  C  |  G  |  A  |  T  |
| : - | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0** |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |  0  |
| **A** |  0  |  1  |  1  |  1  |  1  |  1  |  1  |  1  |  1  |  1  |  1  |  1  |  1  |  1  |  1  |
| **A** |  0  |  1  |  1  |  1  |  1  |  2  |  2  |  2  |  2  |  2  |  2  |  2  |  2  |  2  |  2  |
| **C** |  0  |  1  |  1  |  2  |  2  |  2  |  2  |  3  |  3  |  3  |  3  |  3  |  3  |  3  |  3  |
| **C** |  0  |  1  |  1  |  2  |  2  |  2  |  2  |  3  |  3  |  4  |  4  |  4  |  4  |  4  |  4  |
| **G** |  0  |  1  |  1  |  2  |  3  |  3  |  3  |  3  |  4  |  4  |  5  |  5  |  5  |  5  |  5  |
| **G** |  0  |  1  |  1  |  2  |  3  |  3  |  3  |  3  |  4  |  4  |  5  |  5  |  6  |  6  |  6  |
| **C** |  0  |  1  |  1  |  2  |  3  |  3  |  3  |  4  |  4  |  5  |  5  |  6  |  6  |  6  |  6  |
| **C** |  0  |  1  |  1  |  2  |  3  |  3  |  3  |  4  |  4  |  5  |  5  |  6  |  6  |  6  |  6  |
| **C** |  0  |  1  |  1  |  2  |  3  |  3  |  3  |  4  |  4  |  5  |  5  |  6  |  6  |  6  |  6  |
| **C** |  0  |  1  |  1  |  2  |  3  |  3  |  3  |  4  |  4  |  5  |  5  |  6  |  6  |  6  |  6  |
| **C** |  0  |  1  |  1  |  2  |  3  |  3  |  3  |  4  |  4  |  5  |  5  |  6  |  6  |  6  |  6  |
| **C** |  0  |  1  |  1  |  2  |  3  |  3  |  3  |  4  |  4  |  5  |  5  |  6  |  6  |  6  |  6  |
| **T** |  0  |  1  |  2  |  2  |  3  |  3  |  4  |  4  |  4  |  5  |  5  |  6  |  6  |  6  |  7  |
| **T** |  0  |  1  |  2  |  2  |  3  |  3  |  4  |  4  |  4  |  5  |  5  |  6  |  6  |  6  |  7  |

- The bottom-right cell **c\[14]\[14] = 7**, so the **LCS length is 7**.
- One possible LCS (back-tracking) is

```
A A C C G G T
```

 
 

### **2. b)** Matrix Chain Multiplication


### Matrix Dimensions:

* A = 3×2
* B = 2×4
* C = 4×2
* D = 2×5

Dimension array → `p = [3, 2, 4, 2, 5]`

 

### DP Table **m\[i]\[j]**

`m[i][j]` stores minimum cost (scalar multiplications) to multiply matrices from **Ai to Aj**.

| i\j | 1 | 2  | 3  | 4  |
|   | - | -- | -- | -- |
| 1   | 0 | 24 | 28 | 58 |
| 2   |   | 0  | 16 | 36 |
| 3   |   |    | 0  | 40 |
| 4   |   |    |    | 0  |

 

### **Computation Details**

**m\[1]\[2]:**
Multiply A(3x2) and B(2x4):
`3 * 2 * 4 = 24`

**m\[2]\[3]:**
Multiply B(2x4) and C(4x2):
`2 * 4 * 2 = 16`

**m\[3]\[4]:**
Multiply C(4x2) and D(2x5):
`4 * 2 * 5 = 40`

 

**m\[1]\[3]:** (A(BC)) vs ((AB)C)

* (A)(BC): `0 + 16 + 3*2*2 = 28`
* (AB)(C): `24 + 0 + 3*4*2 = 48`

→ min = **28**

 

**m\[2]\[4]:** (B(CD)) vs ((BC)D)

* (B)(CD): `0 + 40 + 2*4*5 = 80`
* (BC)(D): `16 + 0 + 2*2*5 = 36`

→ min = **36**

 

**m\[1]\[4]:** ((A(BC))D), (A)((BCD)), ((AB)(CD))

* k=1: `(A)(BCD)` → `0 + 36 + 3*2*5 = 66`
* k=2: `(AB)(CD)` → `24 + 40 + 3*4*5 = 124`
* k=3: `(ABC)(D)` → `28 + 0 + 3*2*5 = 58`

→ min = **58**

 

### **Optimal Parenthesization**

From the DP table:

* **m\[1]\[4]** → split at **k=3** → `((A(BC))D)`

 

### Final Answer:

✅ Minimum number of scalar multiplications = **58**
✅ Optimal parenthesization = `((A(BC))D)`

 


### **2. c)** Briefly explain O-notation and Θ-notation with appropriate figure.

**O-notation (Big-O):**

* Upper bound of algorithm runtime.
* T(n) = O(f(n)) → For large n, runtime does not exceed c\*f(n).

**Θ-notation (Theta):**

* Tight bound (both upper and lower).
* T(n) = Θ(f(n)) → For large n, runtime grows exactly like f(n).

**Figure (conceptual):**

```
T(n)
|
|        Θ(n log n)
|        --
|     /        \
| -/          \ -  O(n^2)
|
|_________________________ n →
```

