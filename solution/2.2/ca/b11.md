# Solution of Question-1

---

### (a) Define user CPU time and system CPU time

- **User CPU time** refers to the amount of time the CPU spends executing the instructions of a program, excluding time spent on OS activities. It's the time directly associated with the program’s execution.

- **System CPU time** is the total time the CPU spends performing system-level tasks on behalf of the program. This includes tasks like handling system calls, input/output operations, and other operating system functions that the program indirectly uses.

---

### (b) Differentiate between system performance and CPU performance

| **Aspect**       | **System Performance**                                                                                                           | **CPU Performance**                                                                                                                         |
| ---------------- | -------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- |
| **Definition**   | Refers to how well the entire computer system works, including hardware, software, memory, I/O devices, and network performance. | Refers to how efficiently the CPU performs tasks, typically measured in terms of clock speed, instruction throughput, and processing power. |
| **Scope**        | Broader, includes all system components and user experience.                                                                     | Narrower, focused solely on the CPU’s ability to process instructions.                                                                      |
| **Measurement**  | Can be evaluated using benchmarks that consider system tasks like boot time, multitasking efficiency, and responsiveness.        | Measured by clock speed (GHz), instructions per cycle (IPC), or FLOPS (floating-point operations per second).                               |
| **Factors**      | Affected by factors like memory speed, storage speed, I/O throughput, and network latency.                                       | Affected by clock speed, instruction set architecture (ISA), number of cores, and other CPU-specific features.                              |
| **Optimization** | Can be improved through better system design, faster storage, more RAM, or improved OS and software.                             | Can be improved through faster clock speeds, better instruction pipelining, more cores, or multi-threading.                                 |

---

### (c) Can’t we make CPU faster with faster clock?

Increasing the clock speed of the CPU can make it faster, but **only to a certain extent**. The performance improvement will eventually be limited by factors like:

- **Heat Dissipation**: Higher clock speeds generate more heat, which can lead to thermal throttling and potential damage if not properly managed.

- **Power Consumption**: The CPU requires more power at higher clock speeds, which can make it inefficient in terms of energy consumption.

- **Other Bottlenecks**: If other components like memory or I/O devices are slower, increasing the CPU clock speed won’t have a significant effect on overall system performance.

Thus, **clock speed alone is not a panacea for improving CPU performance**. Other architectural improvements (e.g., more cores, better instruction pipelining) are also required.

---

### (d) Our favourite program runs in 10 seconds on computer A, which has a 2 GHz clock. We are trying to help a computer designer build a computer, B, which will run this program in 6 seconds. The designer has determined that a substantial increase in the clock rate is possible, but this increase will affect the rest of the CPU design, causing computer B to require 1.2 times as many clock cycles as computer A for this program. What clock rate should we tell the designer to target?

#### Solution:

We know the following:

- Time on computer A = 10 seconds
- Clock rate of computer A = 2 GHz (which is 2 × 10⁹ Hz)
- Time on computer B = 6 seconds
- Computer B requires 1.2 times as many clock cycles as computer A.

We will use the formula for performance, which relates execution time, clock cycles, and clock rate:

$$
\text{Execution Time} = \frac{\text{Clock Cycles}}{\text{Clock Rate}}
$$

For computer A:

$$
\text{Clock Cycles}_A = \text{Execution Time}_A \times \text{Clock Rate}_A = 10 \, \text{seconds} \times 2 \times 10^9 \, \text{Hz} = 2 \times 10^{10} \, \text{cycles}
$$

For computer B, the number of clock cycles will be:

$$
\text{Clock Cycles}_B = 1.2 \times \text{Clock Cycles}_A = 1.2 \times 2 \times 10^{10} = 2.4 \times 10^{10} \, \text{cycles}
$$

Since we know the time for computer B is 6 seconds, we can calculate the clock rate for computer B:

$$
\text{Clock Rate}_B = \frac{\text{Clock Cycles}_B}{\text{Execution Time}_B} = \frac{2.4 \times 10^{10}}{6} = 4 \times 10^9 \, \text{Hz}
$$

Thus, the clock rate for computer B should be **4 GHz** to meet the 6-second target.

---

### (e) Why does the clock cycle play a crucial role in sequential circuits?

The clock cycle is crucial in **sequential circuits** because it:

1. **Synchronizes Events**: In sequential circuits, the output depends on the current state and the previous input. The clock signal ensures that all components of the circuit work in sync, allowing the state to change only at precise moments.

2. **Controls Timing**: The clock cycle defines the intervals during which different parts of the circuit can perform operations, ensuring that data is processed in an orderly fashion.

3. **Enables State Transitions**: In sequential logic, such as flip-flops or registers, the clock signal triggers state transitions, making the circuit behave predictably over time.

4. **Prevents Data Corruption**: Without a proper clock cycle, data could be read or written at incorrect times, leading to corruption or inconsistency in the operation of the circuit.

Thus, the **clock cycle** regulates timing, ensures synchronization, and is critical for maintaining correct operation in sequential logic circuits.

---

# Solution of Question-2

---

### **(a) Differentiate between Big-Endian and Little-Endian storage method**

| **Aspect**                 | **Big-Endian**                                                                     | **Little-Endian**                                                                |
| -------------------------- | ---------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |
| **Definition**             | The most significant byte (MSB) is stored first (at the lowest memory address).    | The least significant byte (LSB) is stored first (at the lowest memory address). |
| **Storage order**          | Higher order bytes are stored at lower memory addresses.                           | Lower order bytes are stored at lower memory addresses.                          |
| **Example**                | For 0x12345678, it is stored as 12 34 56 78.                                       | For 0x12345678, it is stored as 78 56 34 12.                                     |
| **Used in**                | Motorola processors, Network protocols (e.g., IP)                                  | Intel processors, x86 architecture                                               |
| **Interpretation of data** | Easier to read in human-friendly format in networks and higher-level applications. | Used for efficiency in computation and data processing in modern computing.      |

---

### **(b) Which computer is faster for this program and by how much?**

Let's calculate the execution time for both computers A and B using the formula:

$$
\text{Execution Time} = \text{Clock Cycle Time} \times \text{CPI} \times \text{Instruction Count}
$$

- **For Computer A**:

  - Clock cycle time = 250 ps
  - CPI = 2.0
  - We are not given the exact instruction count directly, but we are comparing clock cycle times, so let's use this for comparison.

$$
\text{Execution Time}_A = 250 \, \text{ps} \times 2.0 = 500 \, \text{ps}
$$

- **For Computer B**:

  - Clock cycle time = 500 ps
  - CPI = 1.2

$$
\text{Execution Time}_B = 500 \, \text{ps} \times 1.2 = 600 \, \text{ps}
$$

**Conclusion**:

- **Computer A** is faster because 500 ps < 600 ps.
- **By how much**: 600 ps - 500 ps = **100 ps** faster.

---

### **(c) Which code sequence executes the most instructions? Which will be faster? What is the CPI for each sequence?**

We are given two code sequences with instruction counts and CPIs:

| **Code Sequence** | **Instruction Counts for Each Instruction Class** | **CPI for Each Class** | **Execution Time (for each sequence)**                  |
| ----------------- | ------------------------------------------------- | ---------------------- | ------------------------------------------------------- |
| **Sequence 1**    | A = 2, B = 1, C = 2                               | A = 1, B = 2, C = 3    | $2 \times 1 + 1 \times 2 + 2 \times 3 = 2 + 2 + 6 = 10$ |
| **Sequence 2**    | A = 4, B = 1, C = 1                               | A = 1, B = 2, C = 3    | $4 \times 1 + 1 \times 2 + 1 \times 3 = 4 + 2 + 3 = 9$  |

#### **Which sequence executes the most instructions?**

- **Sequence 1** executes 10 instructions, whereas **Sequence 2** executes 9 instructions.
- Thus, **Sequence 1** executes the most instructions.

#### **Which sequence will be faster?**

To calculate which sequence will be faster, we use the formula:

$$
\text{Execution Time} = \text{CPI} \times \text{Instruction Count} \times \text{Clock Cycle Time}
$$

Since the clock cycle times for both sequences are the same, we can directly compare the total CPI \* instruction count values.

- For **Sequence 1**, the execution time is $10$.
- For **Sequence 2**, the execution time is $9$.

Therefore, **Sequence 2** will be faster because it has a lower execution time.

#### **CPI for each sequence:**

- **Sequence 1 CPI**: The CPIs are given directly for Sequence 1 as:

  - A = 1, B = 2, C = 3.

- **Sequence 2 CPI**: The CPIs are given directly for Sequence 2 as:

  - A = 1, B = 2, C = 3.

---

### Final Table Summary

| **Aspect**                  | **Sequence 1**      | **Sequence 2**      |
| --------------------------- | ------------------- | ------------------- |
| **Instruction Counts**      | A = 2, B = 1, C = 2 | A = 4, B = 1, C = 1 |
| **CPI**                     | A = 1, B = 2, C = 3 | A = 1, B = 2, C = 3 |
| **Total Instruction Count** | 10                  | 9                   |
| **Execution Time**          | 10                  | 9                   |
| **Which Executes More**     | Sequence 1          | Sequence 2          |
| **Which is Faster**         | Sequence 2          | Sequence 2          |
| **CPI for Each Sequence**   | A = 1, B = 2, C = 3 | A = 1, B = 2, C = 3 |

---

# Solution of Question-3

---

### **(a) MIPS Assembly Code for the Given C Loop**

We are given the C loop:

```c
while (save[i] == k)
    i++;
```

In MIPS:

- The array **save** is represented by the register **\$s6** (which contains the base address of the array).
- **i** corresponds to **\$s3**.
- **k** corresponds to **\$s4**.

In MIPS, we need to:

1. Load the value from **save\[i]** into a register.
2. Compare **save\[i]** with **k**.
3. If they are equal, increment **i**.

**MIPS Assembly Code**:

```asm
    lw    $t0, 0($s6)      # Load save[i] into $t0 (t0 = save[i])
    beq   $t0, $s4, loop    # If save[i] == k, jump to loop
    addi  $s3, $s3, 1       # Increment i (i = i + 1)
loop:
```

Explanation:

- `lw` loads the value of **save\[i]** into register **\$t0**.
- `beq` compares the value in **\$t0** (save\[i]) with **\$s4** (k). If they are equal, it jumps to `loop`.
- `addi` increments the value of **\$s3** (i).

---

### **(b) Values of Registers After the MIPS Instructions**

We are given the following binary numbers for registers **\$s0** and **\$s1**:

- **\$s0** has the binary number:

  ```
  1111 1111 1111 1111 1111 1111 1111 1111 (0xFFFFFFFF)
  ```

- **\$s1** has the binary number:

  ```
  0000 0000 0000 0000 0000 0000 0000 0001 (0x00000001)
  ```

The MIPS instructions are:

1. `slt  $t0, $s0, $s1` (Set \$t0 to 1 if \$s0 < \$s1, else set \$t0 to 0)
2. `sltu $t1, $s0, $s1` (Set \$t1 to 1 if unsigned \$s0 < \$s1, else set \$t1 to 0)

#### Step-by-Step Execution:

1. **slt \$t0, \$s0, \$s1**:

   - **\$s0** is **0xFFFFFFFF** (signed value = -1).
   - **\$s1** is **0x00000001** (signed value = 1).
   - Since **-1** (value of **\$s0**) is less than **1** (value of **\$s1**), the result is **1**.
   - **\$t0** will be set to **1**.

2. **sltu \$t1, \$s0, \$s1**:

   - In **unsigned comparison**, **0xFFFFFFFF** is treated as a large positive number (4,294,967,295), and **0x00000001** is treated as 1.
   - Since **4,294,967,295** (value of **\$s0**) is greater than **1** (value of **\$s1**) in unsigned comparison, the result is **0**.
   - **\$t1** will be set to **0**.

Thus, after the instructions:

- **\$t0 = 1**
- **\$t1 = 0**

---

### **(c) MIPS Execution Sequence for a Program**

To write the execution sequence of a program using MIPS instructions, we need a specific program or set of operations. However, since the problem doesn't provide a complete code segment for this question, a generic sequence could look something like the following:

1. **Load data into registers**:

   ```asm
   lw    $t0, 0($s6)  # Load word from memory at address in $s6 into $t0
   ```

2. **Perform computation or comparison**:

   ```asm
   add   $t1, $t0, $s4  # Add value in $t0 and $s4, store result in $t1
   slt   $t2, $t0, $s4  # Set $t2 to 1 if $t0 < $s4, else set to 0
   ```

3. **Store result back to memory**:

   ```asm
   sw    $t1, 4($s6)  # Store the value in $t1 to memory at the address in $s6 + 4
   ```

4. **Jump or branch**:

   ```asm
   beq   $t2, $zero, exit  # Branch to exit if $t2 is zero
   ```

This sequence involves loading data from memory, performing computations, storing results, and branching based on conditions.

---

# Solution of Question-4

---

### **(a) Purpose of Program Counter and the Only MIPS Instruction That Can Access It**

- **Program Counter (PC)**:
  The **Program Counter** holds the memory address of the next instruction to be fetched for execution. As each instruction is executed, the PC is incremented to point to the next instruction in the sequence.

- **MIPS Instruction Accessing the PC**:
  The **only MIPS instruction** that can directly access the value of the **Program Counter** is the `JAL` (Jump and Link) instruction. This instruction stores the address of the next instruction (the value in the Program Counter) into the **\$ra** register (return address), so the program can return to it after a function call.

---

### **(b) Define Hit Time and Miss Penalty**

| **Term**         | **Definition**                                                                                                                                                                                                                            |
| ---------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Hit Time**     | The **hit time** is the amount of time required to access a cache when the data requested is found in the cache. It is the time it takes to retrieve data from the cache, excluding any time for accessing main memory.                   |
| **Miss Penalty** | The **miss penalty** is the amount of time required to access the main memory to retrieve data when it is not found in the cache. It includes the time to fetch the data from the slower memory (main memory) and load it into the cache. |

---

### **(c) How Many Total Bits Are Required for a Direct-Mapped Cache with 16 KByte of Data and 4-Word Blocks, Assuming a 32-bit Address?**

Let’s calculate the number of bits required for a **direct-mapped cache**:

- **Cache Size (Data)**: 16 KByte = 16 × 1024 = 16,384 bytes
- **Block Size**: 4 words = 4 × 4 bytes = 16 bytes
- **Address Size**: 32-bit = 4 bytes (because 32 bits = 4 bytes)

To determine the number of bits required, we need to compute:

1. **Number of blocks in the cache**:

   $$
   \text{Number of blocks} = \frac{\text{Cache Size}}{\text{Block Size}} = \frac{16,384}{16} = 1024 \text{ blocks}
   $$

2. **Index Bits** (to address 1024 blocks):

   $$
   \text{Index bits} = \log_2 (1024) = 10 \text{ bits}
   $$

3. **Block Offset Bits** (to address 16 bytes per block):

   $$
   \text{Block offset bits} = \log_2 (16) = 4 \text{ bits}
   $$

4. **Tag Bits** (remaining bits of the address):
   Since the address is 32 bits, the total number of bits for the address is split into:

   - **Tag bits** = 32 (total address bits) - 10 (index bits) - 4 (block offset bits) = 18 bits

Thus, the total number of bits required for a **direct-mapped cache** is the sum of the **Tag bits**, **Index bits**, and **Block offset bits**:

$$
\text{Total bits} = 18 \text{ (Tag bits)} + 10 \text{ (Index bits)} + 4 \text{ (Block offset bits)} = 32 \text{ bits}
$$

---

### **(d) Write Buffer and Write-Back Mechanism for Handling Cache Miss**

- **Write Buffer**:
  A **write buffer** is a temporary storage used to hold data that needs to be written to the cache or main memory. It helps in **handling write misses**, enabling the processor to continue executing instructions while waiting for the data to be written to memory, thus improving performance.

- **Write-Back Mechanism**:
  In the **write-back** cache mechanism, when data is written to a cache location, it is not immediately written back to main memory. Instead, the cache block is marked as "dirty" (modified). When that cache block is replaced (evicted), the modified data is written back to main memory at that time. This reduces the number of write operations to the main memory and improves performance, particularly in scenarios with frequent writes.

---

### Final Summary:

| **Part** | **Explanation**                                                                                                                                                            |
| -------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| (a)      | **Program Counter** holds the next instruction address, and `JAL` is the only instruction that can access the PC.                                                          |
| (b)      | **Hit Time** is the time to retrieve data from the cache, and **Miss Penalty** is the time required to fetch data from main memory when a cache miss occurs.               |
| (c)      | A **direct-mapped cache** with 16 KBytes of data, 4-word blocks, and 32-bit addresses requires **32 bits**.                                                                |
| (d)      | A **write buffer** temporarily stores data to be written to memory, and the **write-back mechanism** writes modified data to memory only when the cache block is replaced. |

---

# Solution of Question-5

---

### **(a) Briefly Write About MIPS. What Are Exception and Interrupt?**

#### **MIPS Overview**:

**MIPS** (Microprocessor without Interlocked Pipeline Stages) is a RISC (Reduced Instruction Set Computing) architecture. It was developed for high performance with a small number of simple instructions, which can execute in a single cycle. MIPS processors have a load/store architecture, meaning that only load and store operations access memory directly, while other instructions work with data in registers. MIPS uses a **pipeline** to improve instruction throughput by overlapping the execution of multiple instructions.

#### **Exceptions**:

An **exception** is an event that occurs during the execution of an instruction that requires the processor to stop its current execution and handle the event. Exceptions can be caused by software (like system calls or errors) or hardware (like invalid memory access or arithmetic errors). After handling an exception, the processor may resume execution from a specific address.

#### **Interrupts**:

An **interrupt** is a signal from external hardware or software that temporarily halts the processor’s current operation, causing it to switch to an interrupt handler to service the interrupt. Interrupts are usually used to handle asynchronous events like I/O operations, timers, or user inputs. After handling the interrupt, the processor resumes its normal execution.

---

### **(b) What is Spilling? Explain Virtual Memory.**

#### **Spilling**:

**Spilling** occurs in the context of register allocation during compilation. It refers to when the number of variables that need to be kept in registers exceeds the number of available registers. When this happens, the compiler "spills" some of the values from registers to the stack or memory, to free up registers for other variables. This is usually done to ensure that the program doesn't run out of available registers during execution.

#### **Virtual Memory**:

**Virtual memory** is a memory management technique that provides an "idealized" abstraction of the storage resources that are actually available on a given machine, enabling a program to address a large range of memory addresses than what is physically available. Virtual memory allows for:

- **Isolation**: Each process operates in its own address space, which helps in security and stability.
- **Paging**: The memory is divided into blocks (pages), and only the pages currently needed are loaded into physical memory.
- **Swapping**: When physical memory is full, pages that are not needed can be swapped to disk.

The operating system handles the mapping between virtual addresses and physical addresses through a **page table**.

---

### **(c) Describe Mapping Functions of Memory Blocks with Figure**

**Memory Mapping** refers to how the addresses of memory locations (in virtual memory) are mapped to physical addresses in the main memory.

#### **Mapping Function**:

In memory mapping, a system uses **page tables** to translate virtual addresses to physical addresses. The mapping typically involves:

- **Virtual Address**: The address that a program sees and works with.
- **Physical Address**: The actual address in the physical memory (RAM).
- **Page Table**: A data structure that maps the virtual addresses to physical addresses.

The process typically includes:

1. The **CPU** generates a **virtual address**.
2. The **Memory Management Unit (MMU)** uses the page table to translate the virtual address into a **physical address**.
3. The physical address is then used to access the data in **physical memory**.

#### **Diagram Explanation**:

Here’s a simplified version of how the **virtual address** is divided for mapping to physical memory:

```
+-----------------+-----------------+-----------------+
|  Virtual Page   |  Frame Number   |  Offset within  |
|     Number      |                 |     Page        |
+-----------------+-----------------+-----------------+

1. Virtual Address: The virtual address is split into a **Page Number** and an **Offset**.
2. The **Page Number** is used to index into the **Page Table**, which gives the **Frame Number**.
3. The **Frame Number** is combined with the **Offset** to produce the **Physical Address**.
```

In a **paging system**, memory is divided into fixed-size pages (in virtual memory), and physical memory is divided into **frames** of the same size. The **page table** maintains a map of which virtual page is stored in which physical frame.

---

### Final Summary:

| **Part** | **Explanation**                                                                                                                                                                                                                                                |
| -------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| (a)      | **MIPS** is a RISC architecture that uses pipelines for fast execution. **Exceptions** are errors or events that require immediate handling, while **Interrupts** are signals from hardware/software to temporarily halt normal processing for event handling. |
| (b)      | **Spilling** occurs when there are more variables than registers, causing some data to be stored in memory. **Virtual memory** allows programs to use more memory than available by swapping data in and out of physical memory.                               |
| (c)      | **Memory mapping** uses page tables to map virtual addresses to physical addresses, ensuring the correct retrieval of data from memory.                                                                                                                        |

---

# Solution of Question-6

---

### **(a) Misses for Different Cache Organizations**

We are given three caches, each with four one-word blocks. The cache organizations are:

1. **Fully Associative Cache**
2. **Two-Way Set-Associative Cache**
3. **Direct-Mapped Cache**

We are tasked with finding the number of misses for each organization given the sequence of block addresses: **0, 8, 6, 8**.

#### **1. Fully Associative Cache**:

In a fully associative cache, a block can be placed in any cache location. So, we just need to keep track of the blocks that are in the cache.

- Initially, the cache is empty.
- **Block 0**: **Miss** (load block 0 into cache).
- **Block 8**: **Miss** (load block 8 into cache).
- **Block 6**: **Miss** (load block 6 into cache).
- **Block 8**: **Hit** (block 8 is already in the cache).

So, the total number of **misses** for a fully associative cache is **3**.

#### **2. Two-Way Set-Associative Cache**:

In a two-way set-associative cache, the cache is divided into 2 sets, and each set can hold 2 blocks. We have 4 blocks total, so the two sets can hold:

- Set 0: Block 0, Block 8

- Set 1: Block 6

- **Block 0**: **Miss** (load into set 0).

- **Block 8**: **Miss** (load into set 1).

- **Block 6**: **Miss** (load into set 0).

- **Block 8**: **Hit** (block 8 is already in set 1).

So, the total number of **misses** for a two-way set-associative cache is **3**.

#### **3. Direct-Mapped Cache**:

In a direct-mapped cache, each block is mapped to a specific cache location. We have 4 cache lines, so each address maps to a specific line. The block addresses map as follows:

- **Address 0** maps to cache line 0.

- **Address 8** maps to cache line 0.

- **Address 6** maps to cache line 2.

- **Block 0**: **Miss** (load into cache line 0).

- **Block 8**: **Miss** (replaces block 0 in cache line 0).

- **Block 6**: **Miss** (load into cache line 2).

- **Block 8**: **Miss** (replaces block 6 in cache line 0).

So, the total number of **misses** for a direct-mapped cache is **4**.

#### **Summary Table**:

| **Cache Organization**        | **Number of Misses** |
| ----------------------------- | -------------------- |
| Fully Associative Cache       | 3                    |
| Two-Way Set-Associative Cache | 3                    |
| Direct-Mapped Cache           | 4                    |

---

### **(b) Add the Numbers 0.5 and -0.4375 in Binary Using Floating Point Addition Algorithm**

#### **Step-by-step Floating Point Addition**:

1. **Convert the numbers to binary (IEEE 754 single precision)**:

   - **0.5** = **0.1** in binary (with an exponent of -1).

     - 0.5 in binary is `0.1` (normalized to `1.0 * 2^-1`).

   - **-0.4375** = **-0.0111** in binary (with an exponent of -2).

     - -0.4375 in binary is `-0.0111` (normalized to `-1.11 * 2^-2`).

2. **Align exponents**:

   - To add these numbers, the exponents must be the same.
   - The first number **0.5** is represented as `1.0 * 2^-1`, and the second number **-0.4375** is represented as `-1.11 * 2^-2`.
   - To align them, shift the second number to match the exponent of **0.5**:

     - `-0.4375 = -1.11 * 2^-2 = -0.111 * 2^-1`.

3. **Add the mantissas**:

   - Now that the exponents are aligned, add the mantissas:

     - $1.0 + (-0.111) = 0.001$.

4. **Adjust the result**:

   - The result is **0.001 \* 2^-1**, which simplifies to **0.001** (or **1 \* 2^-3** in normalized form).

Thus, the result of adding **0.5** and **-0.4375** in binary floating-point representation is approximately **0.0625** or **1.0 \* 2^-4**.

---

### **(c) Multiply $2_{10} \times 3_{10}$, or $0010_2 \times 0011_2$ Using Sequential Multiplication Algorithm**

We use **sequential multiplication** (also known as **shift-and-add multiplication**) to multiply these two binary numbers.

#### **Step-by-Step Multiplication**:

We are multiplying $2_{10}$ and $3_{10}$, or $0010_2$ and $0011_2$.

1. **Set up the multiplication**:

   - $0010_2$ (2 in decimal) and $0011_2$ (3 in decimal).

2. **Initial Values**:

   - Multiplicand: **2** ($0010_2$)
   - Multiplier: **3** ($0011_2$)

3. **Shift-and-add algorithm**:

   - Step 1: $0010_2 \times 1 = 0010_2$, add it to the result (which is initially 0).
   - Step 2: Shift multiplicand left by 1 (resulting in $0100_2$) and multiply by the next bit of the multiplier.
   - Step 3: $0100_2 \times 1 = 0100_2$, add this to the result from step 1.
   - Final result: $0010_2 + 0100_2 = 0110_2$, which equals $6_{10}$.

Thus, $2_{10} \times 3_{10} = 6_{10}$, and the binary result is $0110_2$.

---

### Final Summary:

| **Part** | **Explanation**                                                                                            |
| -------- | ---------------------------------------------------------------------------------------------------------- |
| (a)      | **Fully Associative**: 3 misses, **Two-Way Set-Associative**: 3 misses, **Direct-Mapped**: 4 misses.       |
| (b)      | The binary floating point sum of **0.5** and **-0.4375** is approximately **0.0625** (or **1.0 \* 2^-4**). |
| (c)      | The result of multiplying $2_{10} \times 3_{10}$ using sequential multiplication is $6_{10}$, or $0110_2$. |

Let me know if you need further clarifications or additional details!

---

# Solution of Question-7

---

### **(a) Perform Multiplication Using Booth’s Algorithm for 5-bit Numbers**

We are given:

- **Multiplicand (M)** = `00010` (which is 2 in decimal)
- **Multiplier (Q)** = `11011` (which is -5 in decimal, using two's complement)

We need to perform multiplication using **Booth’s algorithm** and show each step clearly with the Booth recoding.

**Step-by-step Booth's Algorithm Process**:

1. **Initial Setup**:

   - Multiplicand (M) = `00010` (2 in decimal)
   - Multiplier (Q) = `11011` (-5 in decimal, in two's complement)
   - Q-1 (initial value) = `0` (used to keep track of the previous least significant bit of Q)
   - Number of bits = 5 (as specified in the question)

We will also use a register of size 6 bits to handle the results, where the left half will store the partial sum and the right half will store the product.

- **Initial values**:

  - A (Accumulator) = `000000`
  - Q = `11011`
  - Q-1 = `0`
  - M = `00010`
  - -M = two's complement of M = `11110`

2. **Step 1: Examine the Q0 and Q-1 values**:

   - Q0 = 1, Q-1 = 0 → **Subtract M** from A (since the current pair is `10`).
   - A = A - M = `000000 - 00010 = 111110` (using two’s complement subtraction).
   - Perform an **Arithmetic Right Shift (ARS)**: Shift both A and Q right by 1 bit, and make Q0 the sign bit of A.

After shifting:

- A = `111111`
- Q = `11101`
- Q-1 = `1`

3. **Step 2: Examine the new Q0 and Q-1 values**:

   - Q0 = 1, Q-1 = 1 → **No operation**, just perform an **ARS**.

After shifting:

- A = `111111`
- Q = `11110`
- Q-1 = `1`

4. **Step 3: Examine the new Q0 and Q-1 values**:

   - Q0 = 0, Q-1 = 1 → **Add M** to A (since the current pair is `01`).
   - A = A + M = `111111 + 00010 = 000001`.

After shifting:

- A = `000000`
- Q = `11101`
- Q-1 = `0`

5. **Step 4: Examine the new Q0 and Q-1 values**:

   - Q0 = 1, Q-1 = 0 → **Subtract M** from A (since the current pair is `10`).
   - A = A - M = `000000 - 00010 = 111110` (using two’s complement subtraction).

After shifting:

- A = `111111`
- Q = `11110`
- Q-1 = `1`

After 4 steps, the final result is stored in A and Q. We obtain **A = 111111** and **Q = 11110**, which is the result in two's complement. This is the product of **2 × -5 = -10** in decimal.

In table View:

| **Step**    | **A (Accumulator)** | **Q (Multiplier)**               | **Q-1** | **Action**                                                    | **Booth's Operation**                 |
| ----------- | ------------------- | -------------------------------- | ------- | ------------------------------------------------------------- | ------------------------------------- |
| **Initial** | `000000` (0)        | `11011` (-5 in two's complement) | `0`     | Initialize registers.                                         | -                                     |
| **Step 1**  | `111110` (-2)       | `11011` (-5)                     | `0`     | **Q0 = 1, Q-1 = 0 → Subtract M from A**. Perform `A = A - M`. | A = `A - M = 000000 - 00010 = 111110` |
| **Shift 1** | `111111` (-1)       | `11101` (-3)                     | `1`     | Perform **Arithmetic Right Shift** (ARS).                     | ARS: A = `111111`, Q = `11101`        |
| **Step 2**  | `000001` (1)        | `11101` (-3)                     | `1`     | **Q0 = 1, Q-1 = 1 → No operation**, just ARS.                 | ARS: A = `000001`, Q = `11110`        |
| **Shift 2** | `000000` (0)        | `11110` (-2)                     | `1`     | Perform **ARS** again.                                        | ARS: A = `000000`, Q = `11110`        |
| **Step 3**  | `111110` (-2)       | `11110` (-2)                     | `1`     | **Q0 = 0, Q-1 = 1 → Add M to A**. Perform `A = A + M`.        | A = `A + M = 000000 + 00010 = 111110` |
| **Shift 3** | `111111` (-1)       | `11111` (-1)                     | `0`     | Perform **ARS** again.                                        | ARS: A = `111111`, Q = `11111`        |
| **Step 4**  | `000001` (1)        | `11111` (-1)                     | `0`     | **Q0 = 1, Q-1 = 0 → Subtract M from A**. Perform `A = A - M`. | A = `A - M = 111111 - 00010 = 000001` |
| **Shift 4** | `000000` (0)        | `11110` (-2)                     | `1`     | Perform **ARS** again.                                        | ARS: A = `000000`, Q = `11110`        |

---

### **(b) Explain Structural Data Hazard and How It Can Be Resolved**

#### **What is a Structural Data Hazard?**

A **structural hazard** occurs when two instructions require the same hardware resource at the same time, resulting in a conflict. For example:

- Two instructions may attempt to use the same functional unit (like the ALU or a memory unit) simultaneously, causing a stall.
- In the case of memory, both load and store instructions may attempt to access memory at the same time, which could create a conflict.

#### **How Can Structural Hazards Be Resolved?**

Structural hazards can be resolved by:

1. **Duplicating Resources**: By adding additional hardware resources like multiple ALUs or memory units, we can avoid conflicts.
2. **Pipeline Stalls**: In cases where there is no option but to wait for the required hardware resource, a pipeline stall can be inserted to allow the current instruction to complete before the next instruction begins.
3. **Efficient Scheduling**: The compiler can schedule instructions in a way that minimizes the chance of two instructions needing the same hardware resource at the same time.

---

### **(c) Find the Data Hazards in the MIPS Code Segment and Reorder the Instructions**

#### **MIPS Code**:

```asm
lw $t1, 0($t0)    # Load b into $t1
lw $t2, 4($t0)    # Load c into $t2
add $t3, $t1, $t2 # Add b and c, result in $t3
sw $t3, 12($t0)   # Store result into memory
lw $t4, 8($t0)    # Load d into $t4
add $t5, $t1, $t4 # Add b and d, result in $t5
sw $t5, 16($t0)   # Store result into memory
```

#### **Data Hazards**:

- **Hazard 1**: The instruction `add $t3, $t1, $t2` uses the values from `$t1` and `$t2`, but both `$t1` and `$t2` are loaded in the first two `lw` instructions. The data dependency between the load and the add instructions introduces a **read-after-write (RAW) hazard**.
- **Hazard 2**: The instruction `add $t5, $t1, $t4` uses `$t1` and `$t4`, where `$t1` is written by the first `lw` instruction and `$t4` is written by the `lw` instruction at `8($t0)`.

#### **Reordering the Instructions to Avoid Pipeline Stalls**:

Reorder the instructions to resolve data hazards by allowing enough time for data to be loaded into registers before they are used:

```asm
lw $t1, 0($t0)    # Load b into $t1
lw $t2, 4($t0)    # Load c into $t2
lw $t4, 8($t0)    # Load d into $t4
add $t3, $t1, $t2 # Add b and c, result in $t3
sw $t3, 12($t0)   # Store result into memory
add $t5, $t1, $t4 # Add b and d, result in $t5
sw $t5, 16($t0)   # Store result into memory
```

Here, we have reordered the `lw` instructions so that the values are loaded into registers before they are used in the `add` instructions, reducing data hazards.

---

### Final Summary:

| **Part** | **Explanation**                                                                                                                                                                                    |
| -------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| (a)      | Using Booth's algorithm, multiplication of **2** and **-5** results in **-10** in decimal (or `11111111110` in binary).                                                                            |
| (b)      | **Structural Data Hazard** occurs when multiple instructions compete for the same hardware resources. It can be resolved by duplicating resources, using pipeline stalls, or efficient scheduling. |
| (c)      | The MIPS code has data hazards due to dependencies on registers. By reordering the instructions, we can avoid pipeline stalls and ensure data is available before use.                             |

---

# Solution of Question-8

---

### **(a) Explain Pipelining Strategy for Two-Stage Instruction Pipeline with Figure**

#### **Pipelining Strategy for Two-Stage Instruction Pipeline**:

A **two-stage instruction pipeline** involves two distinct stages in the pipeline, where each instruction passes through the stages sequentially. In a simple 2-stage pipeline, the stages could be:

1. **Fetch**: The instruction is fetched from memory.
2. **Execute**: The instruction is decoded and executed (including any memory access or write-back operations).

In this two-stage pipeline, the idea is to overlap the fetching of one instruction with the execution of the previous instruction. The pipeline works as follows:

- **Stage 1 (Fetch)**: Instruction fetches from memory.
- **Stage 2 (Execute)**: Instruction is decoded, executed, and results are written back to registers or memory.

#### **Pipeline Diagram**:

```
|--------------------|--------------------|
|    Instruction 1   |    Instruction 2   |
|--------------------|--------------------|
|  Fetch Stage 1     |  Fetch Stage 1     |
|--------------------|--------------------|
|  Execute Stage 2   |  Execute Stage 2   |
|--------------------|--------------------|
```

In this simple two-stage pipeline:

- While **Instruction 1** is being executed, **Instruction 2** is being fetched.
- This allows for better throughput as multiple instructions are being processed simultaneously.

---

### **(b) Types of Data Hazards and Brief Explanation**

Data hazards occur when there are dependencies between instructions that are executing in parallel. There are three main types of data hazards:

1. **Read-After-Write (RAW) Hazard** (also known as **True Dependency**):

   - This occurs when an instruction depends on the result of a previous instruction.
   - Example:

     ```
     Instruction 1: ADD R1, R2, R3   # R1 = R2 + R3
     Instruction 2: SUB R4, R1, R5   # R4 = R1 - R5
     ```

     In this case, Instruction 2 needs to read **R1**, which is written by Instruction 1, causing a **RAW hazard**.

2. **Write-After-Write (WAW) Hazard** (also known as **Output Dependency**):

   - This occurs when two instructions write to the same register or memory location.
   - Example:

     ```
     Instruction 1: ADD R1, R2, R3   # R1 = R2 + R3
     Instruction 2: SUB R1, R4, R5   # R1 = R4 - R5
     ```

     Here, both instructions attempt to write to **R1**, which can cause a **WAW hazard**.

3. **Write-After-Read (WAR) Hazard** (also known as **Anti Dependency**):

   - This occurs when an instruction writes to a register that a previous instruction is reading.
   - Example:

     ```
     Instruction 1: ADD R1, R2, R3   # R1 = R2 + R3
     Instruction 2: SUB R2, R4, R5   # R2 = R4 - R5
     ```

     In this case, Instruction 2 writes to **R2**, which is being read by Instruction 1, causing a **WAR hazard**.

---

### **(c) Describe Loop Buffers. Draw the Branch Prediction State Diagram**

#### **Loop Buffers**:

A **loop buffer** is a special type of cache designed to speed up the execution of loops. It stores a small portion of code that is frequently executed, typically the instructions from a loop. This helps to avoid the overhead of fetching instructions from main memory repeatedly. When the processor encounters a loop, it can fetch instructions directly from the loop buffer, improving performance.

#### **Branch Prediction State Diagram**:

Branch prediction aims to predict the outcome of a branch instruction (whether it will be taken or not) to avoid pipeline stalls. A common branch prediction algorithm uses **two-bit saturating counters** that predict whether the branch will be taken or not.

Here is a **state diagram** for a **two-bit branch prediction**:

```
                +--------+        Taken        +--------+
                |  00    | -----------------> |  01    |
                |        |                      |        |
                +--------+                      +--------+
                |  Not Taken                     |  Not Taken
                +--------+        Not Taken      +--------+
                |  10    | <----------------- |  11    |
                |        |                      |        |
                +--------+                      +--------+
                     Taken                         Taken
```

**State Diagram Explanation**:

- **State 00**: Predict not taken. If the branch is taken, the state moves to **01**.
- **State 01**: Weakly predict taken. If the branch is taken again, the state moves to **11**. If it’s not taken, the state moves back to **00**.
- **State 10**: Weakly predict not taken. If the branch is not taken again, the state moves to **00**. If taken, the state moves to **01**.
- **State 11**: Predict taken. If the branch is not taken, the state moves to **10**.

This diagram helps the processor decide on branch behavior early, which reduces the penalty of pipeline stalls when branches are encountered.

---

### Final Summary:

| **Part** | **Explanation**                                                                                                                                             |
| -------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------- |
| (a)      | **Two-Stage Pipeline**: Fetch and execute stages work in parallel, improving throughput.                                                                    |
| (b)      | **Types of Data Hazards**: RAW, WAW, and WAR hazards explain dependencies that can stall the pipeline.                                                      |
| (c)      | **Loop Buffers** speed up execution by storing frequently executed instructions; **Branch Prediction** uses a state diagram for predicting branch behavior. |
