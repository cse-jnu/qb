

## 1. Mapping Function: Memory → Cache

**Definition:**
A **mapping function** determines exactly **which cache line** a given main-memory block may occupy.  It is the hardware rule that translates a block number $B$ into a cache line/frame number $L$.

**Why It’s Important:**

* **Simplicity of Lookup:** The CPU need only examine one or a few cache lines per reference.
* **Hardware Cost:** A simple mapping means smaller, faster comparators and tag RAM.
* **Performance (Hit Rate):** The choice of mapping (direct vs. associative vs. set-associative) critically affects how often two hot blocks contend for the same cache line.
* **Predictability:** Operating systems and compilers can tune data layout based on the mapping to reduce conflicts.

---

## 2. Direct-Mapping Addressing & Its Limitations

### 2.1 How Direct Mapping Works

* Let the cache hold $N$ lines (frames), numbered $0$ to $N-1$.
* A main-memory block number $B$ is mapped to cache line

$$
  \text{Line} = B \bmod N.
$$

* Physical address splits into three fields:

  1. **Offset** ($r$ bits) – selects a byte/word within the cache block.
  2. **Index** ($n$ bits) – selects one of the $2^n=N$ cache lines.
  3. **Tag** ($m - n - r$ bits) – stored in the tag RAM to validate a hit.

```
┌────────┬─────────┬─────────┐
|  Tag   | Index   | Offset  |
| m–n–r  |   n     |    r    |
└────────┴─────────┴─────────┘
```

### 2.2 Limitations of Direct Mapping

1. **Conflict Misses:**
   Two frequently used blocks that map to the same line continually evict each other—even if other lines are free.
2. **Poor Utilization in Some Workloads:**
   Streaming or matrix-multiplication patterns often thrash direct-mapped caches.
3. **No Flexibility:**
   A block has exactly **one** possible home; cannot “wander” to an empty line.
4. **Predictability vs. Throughput Trade-off:**
   While hardware is simple, miss rates may be unacceptably high for some applications.

---

## 3. DMA vs. Programmed & Interrupt-Driven I/O

| Mode                           | CPU Involvement                                                                                                      | Pros/Cons                                                          |
| ------------------------------ | -------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------ |
| **Programmed I/O (Polling)**   | CPU continuously tests a status flag, then reads/writes one word at a time.                                          | + Simple<br/>– CPU cycles wasted busy-waiting                      |
| **Interrupt-Driven I/O**       | CPU issues one word transfer, then resumes work; device raises an interrupt per word.                                | + CPU free between transfers<br/>– Many interrupts → high overhead |
| **DMA (Direct Memory Access)** | A DMA controller handles an entire block transfer autonomously; CPU is interrupted **only once** at block start/end. | + Very low CPU overhead<br/>+ High throughput for large transfers  |

### 3.1 Why DMA Is Superior

* **Minimal CPU Overhead:** Instead of $M$ interrupts for an $M$-word transfer, DMA needs only two (start and completion).
* **Higher Throughput:** The bus and memory are used more efficiently, since DMA bursts can transfer entire blocks at full speed.
* **Better CPU Utilization:** CPU can perform other computations while data moves in the background.

### 3.2 Example: Disk Block Read

* **Programmed I/O:**

  ```text
  loop M times:
    poll DISK_STATUS
    CPU ← DISK_DATA   ; one word at a time
  ```

  CPU spends almost all its time polling.

* **Interrupt-Driven:**

  ```text
  for each of M words:
    issue READ_WORD
    wait for INTERRUPT
    on interrupt: CPU ← DISK_DATA
  ```

  Generates $M$ interrupts—still heavy overhead.

* **DMA:**

  ```text
  DMA.setup(source=DISK, dest=Memory[p..p+M-1], count=M)
  DMA.start()
  CPU continues other work
  on DMA_completion_interrupt:
    — one interrupt signals entire block done —
  ```

  CPU overhead drops from $O(M)$ to $O(1)$.

---

## 4. Round-Robin (RR) & Comparison: FCFS vs. SJF

### 4.1 What Is Round-Robin Scheduling?

* A **preemptive** time-sharing algorithm: each ready process receives a fixed **time quantum** $q$.
* Processes are kept in a FIFO ready queue; when a time slice expires, the running process is preempted and placed at the rear of the queue.
* **Fairness:** Every process gets CPU in turn; suitable for interactive systems.
* **Tunable:** Shorter $q$ → more responsive but higher context-switch overhead; longer $q$ → approaches FCFS behavior.

---

### 4.2 FCFS vs. SJF: Differences & Example

| Criterion           | FCFS (First-Come, First-Serve)           | SJF (Shortest Job First)                   |
| ------------------- | ---------------------------------------- | ------------------------------------------ |
| **Type**            | Non-preemptive                           | Can be preemptive (SRTF) or non-preemptive |
| **Decision Basis**  | Arrival order                            | Predicted CPU burst length                 |
| **Waiting-Time**    | Can be large if a long job arrives first | Minimizes average waiting time             |
| **Starvation Risk** | None                                     | Short jobs thrive; long jobs may starve    |
| **Implementation**  | Very simple                              | Requires burst-time estimation             |

#### Example

Three processes arrive at time 0 with CPU bursts as follows:

| Process | Burst (ms) |
| ------- | ---------- |
| P₁      | 10         |
| P₂      | 5          |
| P₃      | 2          |

* **FCFS order:** P₁ → P₂ → P₃

  * Waiting times:

    * P₁: 0
    * P₂: 10
    * P₃: 10 + 5 = 15
  * **Average waiting** = $(0 + 10 + 15)/3 = 8.33\,\text{ms}$

* **SJF order:** P₃ → P₂ → P₁

  * Waiting times:

    * P₃: 0
    * P₂: 2
    * P₁: 2 + 5 = 7
  * **Average waiting** = $(0 + 2 + 7)/3 = 3.0\,\text{ms}$

